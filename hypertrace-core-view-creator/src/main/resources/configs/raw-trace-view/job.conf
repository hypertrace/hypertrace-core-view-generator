mainClass = org.hypertrace.core.viewcreator.pinot.PinotTableCreationTool

view.name = rawTraceView
view.output.schema.class = org.hypertrace.core.viewgenerator.api.RawTraceView
view.output.schema.url = "host:port/myView"

kafka.brokerAddress = "bootstrap:9092"
kafka.topicName = raw-trace-view-events
kafka.partitions = 1
kafka.replicationFactor = 1

pinot.controllerHost = pinot-controller
pinot.controllerPort = 9000
pinot.timeColumn = start_time_millis
pinot.timeUnit = MILLISECONDS
pinot.dimensionColumns = [tenant_id, trace_id, transaction_name, services, start_time_millis, duration_millis, end_time_millis, num_services, num_spans]
pinot.columnsMaxLength = {}
pinot.metricColumns = [num_services, num_spans]
pinot.invertedIndexColumns = []
pinot.rangeIndexColumns = [start_time_millis]
pinot.bloomFilterColumns = []
pinot.noDictionaryColumns = []
pinot.tableName = rawTraceView
pinot.loadMode = MMAP
pinot.numReplicas = 1
pinot.retentionTimeValue = 5
pinot.retentionTimeUnit = DAYS
pinot.brokerTenant = defaultBroker
pinot.serverTenant = defaultServer
pinot.segmentAssignmentStrategy = BalanceNumSegmentAssignmentStrategy

pinot.streamConfigs =
  {
    streamType: kafka,
    stream.kafka.consumer.type: LowLevel,
    stream.kafka.topic.name: raw-trace-view-events,
    stream.kafka.consumer.factory.class.name: "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
    stream.kafka.decoder.class.name: "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",
    stream.kafka.decoder.prop.schema.registry.rest.url: "http://schema-registry-service:8081",
    stream.kafka.hlc.zk.connect.string: "zookeeper:2181",
    stream.kafka.zk.broker.url: "zookeeper:2181",
    stream.kafka.broker.list: "bootstrap:9092",
    realtime.segment.flush.threshold.time: 3600000,
    realtime.segment.flush.threshold.size: 500000,
    stream.kafka.consumer.prop.auto.offset.reset: largest
  }